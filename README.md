## Video-Datasets


### Fall-Down-Detection

<br> [UR Fall Detection Dataset](http://fenix.ur.edu.pl/~mkepski/ds/uf.html):Michal Kępski, Interdisciplinary Centre for Computational Modelling, University of Rzeszow

<br> [Multiple cameras fall dataset](https://www.iro.umontreal.ca/~labimage/Dataset/): This dataset contain 24 scenarios recorded with 8 IP video cameras. The first 22 first scenarios contain a fall and confounding events, the last 2 ones contain only confounding events.

<br> [Video Datasets](http://videodatasets.org/): This web site contains links to a number of video datasets used for computer vision research and created over a number of years by teams working with/in collaboration with  Prof. Sergio A Velastin, professor of Applied Computer Vision, recently a UC3M-Conex Marie Curie Research Professor at the Applied Artificial Intelligence Research Group, Universidad Carlos III de Madrid (Spain) and former director of the Digital Imaging Research Centre (Kingston University London, UK). He is now Visiting Professor at Queen Mary University of London, UK and at Universidad Carlos III de Madrid, Spain

<br> [ViHASi: Virtual Human Action Silhouette Data](http://velastin.dynu.com/VIHASI/): for the Evaluation of Silhouette-Based Action Recognition Methods and the Evaluation of Silhouette-Based Pose Recovery Methods (NEW) (last updated on the 13th of January 2009)

<br> [MuHAVi: Multicamera Human Action Video Data](http://velastin.dynu.com/MuHAVi-MAS/): including selected action sequences with MAS: Manually Annotated Silhouette Data MuHAVi-uncut: Full videos with realistic Silhoutte Data for the evaluation of human action recognition methods (Last updated September 2017) 

<br> [FALL-UP DATASET](https://sites.google.com/up.edu.mx/har-up/): The aim of this project is to assist elderly people using novel human activity recognition machine learning methods. Additional: [Github](https://github.com/jpnm561/HAR-UP)

### Human-Actions

<br> [Kinetics](https://github.com/cvdfoundation/kinetics-dataset): Kinetics is a collection of large-scale, high-quality datasets of URL links of up to 650,000 video clips that cover 400/600/700 human action classes, depending on the dataset version. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 400/600/700 video clips. Each clip is human annotated with a single action class and lasts around 10 seconds.

<br> [GAMING DATASETS](http://velastin.dynu.com/G3D/index.html): The dataset provider captured single and multiplayer gaming datasets for public use, both containing synchronised colour, depth and skeleton data. For an overview of these datasets please see the table below, for more details or to download a dataset please follow the dataset links. Due to the formats selected, it is possible to view all the recorded data and metadata without any special software tools. For compatibility the same formats were used for both datasets. 

<br> [PAMELA UANDES DATASET](http://videodatasets.org/PAMELA-UANDES): The objective of the project is to study passenger behaviour and its relation to the environment, as they get on and off a public transport vehicle, in this case a full-scale metropolitan train model. 

<br> [BOSS DATASET](http://videodatasets.org/BOSSdata): The BOSS dataset is a fairly realistic dataset for pose, action and interaction recognition. It was originally captured as part of the Eureka's Celtic Initiative "BOSS : On Board Wireless Secured Video Surveillance BOSS" project. It consists of video/audio recordings of acted actions inside a moving train. It used multiple calibrated cameras.

<br> [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/): MPII Human Pose dataset is a state of the art benchmark for evaluation of articulated human pose estimation. The dataset includes around 25K images containing over 40K people with annotated body joints. The images were systematically collected using an established taxonomy of every day human activities. Overall the dataset covers 410 human activities and each image is provided with an activity label. Each image was extracted from a YouTube video and provided with preceding and following un-annotated frames. In addition, for the test set we obtained richer annotations including body part occlusions and 3D torso and head orientations. 

<br> [NTU RGB+D](https://rose1.ntu.edu.sg/dataset/actionRecognition/): "NTU RGB+D" is a large-scale dataset for human action recognition. [Github](https://github.com/shahroudy/NTURGB-D) It is introduced in their CVPR 2016 paper [PDF](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.pdf).

### Object

<br> [THE UTUAV URBAN TRAFFIC DATASET](http://videodatasets.org/UTUAV): We present the UTUAV dataset which consists of three different scenes captured in the second largest city of Colombia: Medellín. Road user classes representative of those in emerging countries such as Colombia, have been choosen: motorcycles (MC), light vehicles (LV) and heavy vehicles (HV). 

<br> [MOTOR BIKE DATASET](http://videodatasets.org/UrbanMotorbike): We captured a video sequence datasets for public use, which contain images taken with a Phantom 4® drone, with an HD camera under windy conditions, which affected the image stabilizer capabilities.  Images were resized to 640 x 364 pixels, containing 41,040 to 56,975 ROI annotated objects, with a minimal height size set to 25 pixels. 60% of the annotated data corresponds to occluded motorcycles. Objects partially occluded with height less than 25 pixels were not annotated.

### Mixed

<br> [https://research.google.com/youtube8m/index.html](https://research.google.com/youtube8m/index.html): The YouTube-8M Segments dataset is an extension of the YouTube-8M dataset with human-verified segment annotations. In addition to annotating videos, we would like to temporally localize the entities in the videos, i.e., find out when the entities occur.

